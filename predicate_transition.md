# Predicate transition
## Introduction
Given a query, the database application parses it into an abstract syntax tree(AST), based on which an execution plan is then generated. 

An execution plan is of tree structure; similar to the magma flow during volcanic eruption, data is generated by the leaf node and flows towards the root.
When a non-leaf tree node receives data from its child, it applies some manipulation over it, such as filter out some lines(Selection), 
removing or adding extra columns(Projection), or matching rows between two children(Join). After that, the modified data is returned to its father node.
The whole process is similar to the lava flow during volcanic eruption, data moves from below to above and when the root node returns, the output is send to the client as the query result.
(By the way, stream execution is often used in the process to avoid load all data into main memory.)

![image](https://github.com/Charles-1791/database_knowledge/assets/89259555/3148e37e-92f6-4831-bd81-443f4d7c790f)

Usually, the execution plan derived from AST(raw plan) is just a verbatim translation of the query -- it yeilds the correct output but fails to take into account the execution cost.
In fact, some optimization can be apply to the raw plan to achieve a shorter execution time while maintaning the correctness of the result, for instance, 
join reordering, which changes the join order when more than two tables are joined together, or column pruning, which removes unused columns from the output of each
tree node, in a way reducing the amount of data transaferred among nodes. Here i'd like to share my views on a common but crucial optimization method -- predicate transtition.

## Outline
In this passage, i would firstly give a brief introduction of tranditional predicate push down, and then switch to the concept of predication transition.
After that, some execution plan nodes are introduced, on which we discuss how to perform predicate transition. Finally, some drawbacks and limitations are covered.

## Predicate Pushdown
Predicate pushdown aims to push predicates, i.e. filters or conditions, towards leaf nodes, so that data is filtered as early as possible. 
The rationale is simple: less amount of data means less time spent on transfering data among nodes, and it reduce the pressure of memory. 

Another remarkable advantange is that it preclude unnecessary computation:
Suppose we have table t1 and t2 with the following definition:

***t1***
| a | b | c |
| --- | --- | --- |
| int | double | varchar |

***t2***
| d | e | f |
| --- | --- | --- |
| int | double | varchar |

For query:
> select a, b, d, e from t1 join t2 on b = d and b > 1.0 where f like 'abc%'

the generated RAW plan should be like:

![image](https://github.com/Charles-1791/database_knowledge/assets/89259555/d27055c7-876d-4f89-a4d2-75d144695625)

Two table scan nodes retrieve data of t1 and t2 from disk and return to Join, the Join does a Cartesian Product over rows from its child(in fact, we could do a hash join) and keeps
result meeting the condition b = d and b > 1.0. The join output is then returned to a Selection node(filter), and rows not satifsfying 'f like abc%' is removed. What is left goes to the projection node,
where only columns a, c, d, e are preserved.

If we push the predicates 'b > 1.0' and 'f like 'abc%'' to the left and right table scan nodes, the number of rows received by Join is significantly reduced, which decreases join's computing pressure. 

## Predicate Transition
Predication transtition is an advanced version of traditional predicate push down. Apart from existing predicates, predicate transition derives new predicates and pushes them downwards. In addition, the newly deduced predicates may take the place of the original ones if they are favored by databases(considering indexes). 

Revisit the previous example:

> select a, b, d, e from t1 join t2 on b = d and b > 1.0 where f like 'abc%'

Once we know b = d and b > 1.0, we instantly deduce d > 1.0, which can be pushed to table scan of t2.
Our deduction is based on equivalent relationship(b = d), and this would be the only type of deduction covered in this passage.
The more advance kinds of deductions are:

- a > b and b > 0 so a > 0, (greater-than/less-than relationship is transitive)
- max(a) < 10 so a < 10, (a <= max(a) < 10)
- sum(a) < 10 and a > 0 so a < 10, (mathematics),
...


However, not all derived predicates are meaningful. Assuming the query looks like:

> select a, b, d, e from t1 join t2 on b = d and b > 1.0 where b = a

After seeing b = a, b > 1.0, we know a must also be greater than 1.0. Since column 'a' and 'b' are from the same table, the derived 'a > 1.0' only makes a difference if there is an index on column 'a'.
Such predicates is worthless and slows down the query, in that it introduces needless computation.

In a nut shell, predicates transition deduces new predicates from existing ones, 'expanding' the predicate set, and pushes them downwards. During the process, we should be cautious not to generate trivial predicates; sometimes we replace the existing predicates with newly derivations for better performance.

## Implementation
Different from the approach often adopted in papers, our goal is to 'deduce only when you have to' -- we do not generate predicates as much as possible(predicate closure), pushes all of them and eliminate redundancy; instead, we only deduce nontrivial and pushable predicates.

In the following contents, we would discuss what we should do at each plan node, but before that, let me introduce what a plan node actually is.

### Overviews of plan nodes
A plan node may have two, one or no children node; the leaf node generates data itself, while others receives data from its children. In optimization phase, there's no real data transferred, but each node is clear of the meta infos(column numbers, the name and type of each column) of the data it would receive and return in execution phase. Each column owns a global unique id to distinguish it from others; a tree node takes in some uids from its children, apply some changes over the data or filter out some rows, then it returns some uids.  

After labeling the column, the query:
> select a, b, d, e from t1 join t2 on b = d and b > 1.0 where f like 'abc%'

yields plan tree:
![image](https://github.com/Charles-1791/database_knowledge/assets/89259555/bb5ce754-eb0a-47da-94f8-6a6e80c57180)

According to the input and output uids, one-child-noded can be classified into three catagories:
- output uids are same as input uids. (Limit, Selection, Sort.)
- output contains possibly more uids than input uids(inputUids is a subset of outputUids). (Aggregation and WindowFunction)
- output only preserves some or no uids from input and add more or no uids to its output. (Projection)

node with two children 
- output uids are the combinataion of uids of left and right child. (Join)
- output uids are completely different from any uids from its children. (SetOperation such as. Union, Minus, Intersect...)

### Two phases
Predicate transition can be divided into two phases -- predicate pullup and predicate push down.

During the pullup phase, every node(except the table scan) receives from its child a 'PredicateSummary', which records the arithmatic relationship among the columns. The 'PredicateSummary' is processed in distinct manner according to the nature of the node, then returned to its father node, where further modifications are made. Similary to a bubble rising to the water surface, one by one, the 'PredicateSummary' ascends from the bottom(leaves) to the top(root).

![image](https://github.com/Charles-1791/database_knowledge/assets/89259555/82d6e0ff-49a7-4d89-b259-f787f1542552)

The push down phase ensues end of the pull up phase, and the summary returned from the root is now carried down from root to leaves. When a summary goes through a node, its content changes and new predicates may be generated, meanwhile some predicates remain there and won't go further down. Once a summary reaches the leaf node, i.e. the Table Scan node, it 'flattens' into a group of predicates and are attached to the TableScan.

![image](https://github.com/Charles-1791/database_knowledge/assets/89259555/c6f90a55-02ab-445c-9847-fb59ee8c0c73)

For a certain plan node, if we name as 'summary_up' the predicate summary returned by it during pullup phase, and 'summary_down' the summary it receives from its father during pushdown phase, we always have summary_up <= summary_down, in other words, the summary_down always contains more 'knowledge' or 'information' than summary_up. Another rule that all nodes follow is that a summary returned or received by a particular node must include and exclusively consist of its own output columns.

### Predicate Summary
#### Structure
A predicate constrains the arithmetic relationship among columns, for instance, predicate 'a > 0' add restriction to column a, predicate 'b < c' forces 'c' must be greater than 'b' in result. Another way to see predicates is to consider than as 'promises' or 'data feature', that is, a row won't appear in the result set unless it 'conforms to' all the predicates. 

A predicate summary is a synthesis of multiple predicates, and it consists of two fundamental structures -- a set of all equivalent sets and a list of predicates.

```
struct PredicateSummary{
  EqRelations relations
  Expression[] conditions
}
```

#### relations
A 'relations' is an array containing multiple non-overlapping 'equivalent sets', each comprising several columns that equate to each other.
If we assign each column a unique id like '#1', an equivalent set can be expressed as:

> {#1, #2, #3},

and it indicates 

> #1 = #2 = #3

So a 'relations' can be written as:

> \[ {#1, #2, #3}, {#4}, {#5, #6}, {#7}, {#8, #9} \]

and it demonstrates:

> #1 = #2 = #3, #5 = #6, #8 = #9

A 'relations' also keeps a map mapping each UID to its corresponding set index within the array.
A map looks like:

| UID | set index |
| --- | ---|
| #1 | 0 |
| #2 | 0 |
| #3 | 0 |
| #4 | 1 |
| #5 | 2 |
| #6 | 2 |
| #7 | 3 |
| #8 | 4 |
| #9 | 4 |

#### conditions
'conditions' store's all predicates other than those owning the form 'column1 = column2', which is converted into an equivalent set {column1, column2} and recorded in 'relation'.
A predicate in 'conditions' should be interpreted as 'a relation among equivalent sets', rather than 'a relation among columns'.
Considering the following case:
![image](https://github.com/Charles-1791/database_knowledge/assets/89259555/07754464-9f43-4be2-82d0-818edc837023)

The purple block illstrates a 'relation', based on which the predicate

> #1 + #2 - f(#4) * g(#5) = #8 + #9
 
should be construed as a 'template'

> $0 + $0 - f($1) * g($2) = $3 + $4

where the $ signs are placeholders -- $0 stands for the first equivalent set, $1 stands for the 2nd equivalent set...
In this way, a predicate in fact reveals the arithmetic relations among equivalent sets, and if we replace the placeholders with a column within its corresponding set, a new(and valid) predicate is generated. In the example given above, the template could be used to generate 3 * 3 * 2 * 2 * 1 * 1 = 36 different predicates in total.

### Pullup and Pushdown logic
Since the pullup and pushdown logic strongly correlate, the following content is structured in a pattern where each node is followed by its corresponding pullup and pushdown details.

#### Table Scan
##### Pull up
A table scan node initially does not have any predicates, so the pull up process is quite simple: create an empty summary and add all output uids into summary.relations and return the summary.

##### Push down
Table scan has no child, so all the data feature recorded in summary must be converted back into predicates. 
Similar to stringing some pearls into a necklace, the process of flattening a 'relations' is using equation marks to connect each column, which can be expressed as:
```
for each equivalent set S in relations {
  if |S| == 1 {
    continue
  }
  for i:=1; i < S.size(); ++i {
    generate predicate 'S[i-1] = S[i]'
  }
}
```
The logic behind is quite intuitive, since columns(uids) in an equivalent set equate to each other, we could pick the first element and create an equation connecting it with others. A quick example:

> flatten(\[ {#1}, {#2, #3, #4}, {#5, #6} \]) = \[#2 = #3, #3 = #4, #5 = #6\]


Since 'conditions' itself is an array of predicates which can be used directly, a better practice, nevertheless, would be converting them into new conditions more 'favored' by database through subsituting columns. As metioned above, a predicate reveals relations among equivalent sets; after replacing column with an  indexed(and equivalent) one, a new predicate usually outperforms the original one thanks to the more efficient index scan. The pseudocode is:

```
// we need relations to tell us the equivalent info
func TryToConvert(const expression.Expr& predicate, const EqRelation& relations) (expression.Expr, bool) {
  copied = predciate.copy()
  all_cols = predicate.extract_columns()
  for each col in all_cols {
    if there is index on col {
      continue
    }
    eqSet = relations.GetEquivalentSet(col)
    newCol = pick one indexed column from eqSet
    if newCols is not null {
      copied.replace(col, newCols)
    } 
  }
  return copied
}
```

### Selection
A selection node receives from its child data from which some rows are filered out according to the predicates on it. Since Selection only remove some rows, the columns are not affected, which means the input columns are the same as output columns.
#### Pull up
<img width="640" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/27f11e5d-1185-44b6-a522-cc1fc90beae1">

As a 'filter node', a selection node always has a predicate, which could be converted into CNF(conjunctive normal form: https://en.wikipedia.org/wiki/Conjunctive_normal_form). For each clause in the CNF, we check whether it is of the form 'col1 = col2', if so, we find a pair of equivalent columns <col1, col2>, which is added to the 'relation' field of the PredicateSummary returned from child node; else, we consider the clause a normal predicate and simply move it into 'conditions'.

#### Push down
In push down phase, selection recieved from its father a summary, which contains the predicates the current node used to have. Since selection node must have a child, to which we could directly send the summary.

### Projection
A projection node removes unwanted columns and does computation over one or several columns to generate new columns.
Naming columns returned from projection's child as input columns, and those returned by projection itself as output columns, we can divide these columns into three categories:
- Survivors: columns appear in both input and output
- Victims: columns appear in input but not output
- Newcomers: columns appear in output but not input
<img width="249" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/af4a6671-3721-4216-89f5-367eae7abf70">

For example, in the illstration above, we have:
| category | columns |
|----------|---------|
| survivors | #2, #4 |
| victims | #1, #3 |
| newcomers | #5 |

#### Pull up
During pull up phase, the summary returned from child must contain info(equivalent relation and conditions) about input columns. Since an input column may be a victim, any infos involving such column should be modified or erased from the summary. 

<img width="674" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/382af549-1923-4251-af21-f42dd6826660">

Let start from relation. For each equivalent set in relation, victims should be pruned from it in that the father node has no idea what such columns are(victims are not return to father node, and in execution phase no data of these columns are reieved). Nevertheless, simply removing of victims leads to an irreversible loss of information -- there's no way to add them back in push down phase. For example, in the last example, assuming in child summary we have an equivalent set {#1, #2}, if we just remove #1, when we do push down, the information #1 = #2 is forever lost. Thus, we need to save the equivalent set in a buffer before filtering out the victims.

Conditions contaning victims should not be returned to father node either, but different from victims in relation, it's possible to convert a predicate to make it 'acceptable' by father node, i.e. in a form only contains survivors. Suppose we have '#1 = #2', from predicate '#1 + #4 < 100', we deduce that '#2 + #4 < 100', which is now valid for parent plan. So the general idea is to substitute victims with its equivalent survivors(if possible), if all victims are replaced, the generated predicate can be reported to father, otherwise, move such predicate into buffer.

#### Push down

<img width="199" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/eca2b7b4-854e-45ce-8ff6-2e02388fb994">

##### relation
Again, let's begin with 'relation'. An equivalent set now contains newcomers and survivors, and the newcomers should not appear in the summary sent to child(remember, a summary received by a node must contain exactly its output columns). Removing from the set newcomers again cause information loss, for example, if we have a set {#2, #5}, where #5 is newcomer, erasing #5 from the set causes the loss of data feature '#2 = #5'. 

Fortunately, a newcomer is always an expression comprising exclusively columns from child, i.e. victims + survivors -- the #5 in previous example is defined as #3 + #4. Thus, we could generate a predicate '#2 = #3 + #4' and store it in the Summary.conditions, which would later be sent to child plan. The pseudo-code of such procedure is:
```
colum_from_child = victims + survivors
ret = []
for each equivalent set S in relation {
  if all cols in S in column_from_child
    continue
  else
    S+ = S intersect column_from_child
    S- = S intersect victims
    validCol = S+.first
    for each invalid_col in S-:
      gen = 'invalid_col.corres_expr = validCol'
      ret += gen
}
return ret
```
In short, when handling a relation, some columns must be removed from equivalent sets, meanwhile new predicates are generated and would be pushed down to child. 
At last, we merge the relation after pruning with relation in buffer to draw a comprehensive picture of column equal relationship.
##### condition
A predicate in condition may also contain newcomers, but we could firstly try to replace such columns with their equivalent survivors. For example, if we have:

<img width="224" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/2670c5a4-8eb0-4a3e-a497-a8f8ae2db69c">

| category | columns |
|----------|---------|
| survivors | #1, #2 |
| victims | #4 |
| newcomers | #3, #5 |

and 
relation
> \[{#1}, {#2, #3}, {#5}\]

For predicate #1 + #3 < 100, we could generate #1 + #2 < 100, which is child-acceptable.
But for predicate #1 + #5 = 128, since there is no survivor equivalent to #5, this predicate is transformed into '#1 + (#2 * #1 +100) = 128'.

##### summary

<img width="766" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/06f72d75-b6a1-4453-b991-a0078552c947">

The whole process can be split into two parts -- relation and condition. For relation, we remove newcomers and generate expressions which is added to condition; for condition, we examine each predicate and displace newcomes with equivalent survivors or their expressions. Finally, we merge the relation and condition sperately with the ones in buffer to draw a full picture.

### Limit
#### Pull Up
A limit node keeps certain number of rows received from child without changing their values, so all data features from the child stay true. A natural practice is to return directly to father the child summary, while due to the reason to be discussed in the following content, we save an extra copy of summary in buffer.

#### Push down
One fascinating feature about limit is its semipermeability -- a child summary can be straightly sent to parent node whereas a summary from parent cannot go down further. In our context, summary from father should not go through limit, but stays 'above' the limit. So we should flatten the summary into predicates and create a new selection node to which these predicates are sent. You might have observed a defect -- the summary received from a parent contains at least as much information as the one submitted to it; directly flattening it leads to the inevitable inclusion of duplicate predicates mirroring the ones found in nodes beneath the limit. Let say we have the following two summaries：

<img width="206" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/db96f306-d63f-477f-96d8-3543d1b14abc">

The green block above stands for the summary received by limit in push down phase and the purple block denotes the summary returned by child in pull up phase. 
Apparently, flatten the equivalent set {#1, #2, #3} in the green summary generates #1 = #2 and #2 = #3, whereas #1 = #2 is included in the purple summary. Generating such redundant predicate results in a slower execution, and should be avoided. Meanwhile, the predicate #1 + #3 < 100 is equivalent to #1 + #2 < 100, which mirrors the one in purple summary, so such predicate in condition should also be removed. 

So far, we know that some extra pruning works are necessary after flattening summary, but in fact we could truncate the summary beforehand. 

##### relation

Starting from relation: for limit, an equivalent set in fatherSummary.relation can invariably be segregated into several subsets such that each of them is a complete equivalent set in childSummary.relation.

<img width="805" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/2f53cb4e-59c0-42ff-81b6-58ff8b688df9">

For example, in the above image, the set {#1, #3, #5, #6, #7} is perfectly split into {#1, #3}, {#5}, {#6, #7}. The following case could not possibly happens:

<img width="315" alt="image" src="https://github.com/Charles-1791/database_knowledge/assets/89259555/dd64099a-26bf-48a3-baab-50e8bf4795bc">

An equivalent set in child relation must be fully contained in an equivalent set in father relation. In the example, once #4, #5 are in same set, there's no operation that could tear them apart. As a result, columns in the same set forms an unbreakable group.

In general, for each equal set in fatherSummary.relation, according to the childSummary.relation(this is a reason why we store a copy of child summary in buffer), we split it into several smaller ones. After that, we randomly select a column from all subset(a better approach is to choose columns with index on them), let's say, 

#c<sub>1</sub>, #c<sub>2</sub>, #c<sub>3</sub>, ..., #c<sub>i</sub>, 

and generate predicates 

#c<sub>1</sub> = #c<sub>2</sub>, #c<sub>2</sub> = #c<sub>3</sub>, ..., #c<sub>i-1</sub> = #c<sub>i</sub> 

##### condition
A predicate is redundant when


